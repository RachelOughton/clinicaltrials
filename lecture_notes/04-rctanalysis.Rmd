---
always_allow_html: true
---


# (Lecture 7) Analyzing RCT data {#rct-analysis}

*We're now in the post-trial stage. The trial has been run, and we have lots of data to analyze to try to assess what effect the treatment or intervention has had. In general we will use the notation $\tau$ to denote the treatment effect.*

*In this chapter we'll keep our focus on the scenario where the trial outcome is measured on a continuous scale, but in later weeks we'll go on to look at other types of data.*

We now have trial data, and want to estimate the treatment effect $\tau$.

::: {.example} 
  
  * From @hommel1986effect. 
  * 16 diabetes patients
  * Group $T$ receive Captopril, a drug that may reduce blood pressure.
  * Placebo given to group $C$
  * Primary outcome $X$ is systolic blood pressure (mmHg)

  
*This is important, since for those with diabetes, high blood pressure can exacerbate kidney disease (specifically diabetic nephropathy, a complication of diabetes). To participate in the trial, people had to be insulin-dependent and already affected by diabetic nephropathy. In the trial, systolic blood pressure was measured before participants were allocated to each trial arm, and then measured again after one week on treatment. A placebo was given to the control group, so that all participants were blinded.*

*The baseline and outcome blood pressure measurements (in mmHg) are shown in Table \@ref(tab:captoprildata). We see that nine participants were assigned to the treatment arm (Captopril) and the remaining seven to the placebo group. @hommel1986effect say that the patients were 'randomly allocated' to their group.*


```{r captoprildata}
df_hommel = data.frame(
  patient = c(1:9, 1:7),
  baseline = c(147, 129, 158, 164, 134, 155, 151, 141, 153, 133, 129, 152, 161, 154, 141, 156),
  outcome = c(137, 120, 141, 137, 140, 144, 134, 123, 142, 139, 134, 136, 151, 147, 137, 149),
  arm = c(rep("Captopril", 9), rep("Placebo", 7))
)
df_hommel$arm = as.factor(df_hommel$arm)
df_hommel_print = df_hommel
names(df_hommel_print) = c("Patient (ID)", "Baseline (B)", "Outcome at 1 week (X)", "Trial Arm")

knitr::kable(df_hommel_print,  
             booktabs = TRUE, 
             caption = "Data for the Captopril trial from @hommel1986effect.")

```


*This is very small dataset, and so in that respect it is quite unusual, but its structure is similar to many other trials.*

:::
  
  We will build up from the simplest type of analysis to some more complicated / sophisticated approaches.

## Confidence intervals and P-values {#ttest}

Because the randomization process should produce groups that are comparable, we can in principle compare $X$ between the groups.

::: {.example}
Summary statistics of the outcome for each group are shown below.

```{r}
df_hommel1 = data.frame(
  Sam = c(9,7),
  Mean = c(
    mean(df_hommel$outcome[df_hommel$arm=="Captopril"]),
    mean(df_hommel$outcome[df_hommel$arm=="Placebo"])),
  SD = c(
    sd(df_hommel$outcome[df_hommel$arm=="Captopril"]),
    sd(df_hommel$outcome[df_hommel$arm=="Placebo"]))
)
df_hommel1$SEmean = df_hommel1$SD / sqrt(df_hommel1$Sam) 
df_hommel1_print=  round(df_hommel1,2)
names(df_hommel1_print) = c("Sample Size", "Mean (mmHg)", "SD (mmHg)", "SE of mean (mmHg)")
row.names(df_hommel1_print) = c("Captopril", "Placebo")

knitr::kable(df_hommel1_print, caption = "Summary statistics for each group.")

```

We have 

$$ 
\begin{aligned}
\bar{x}_T & = 135.33 \\
\bar{x}_C & = 141.86.
\end{aligned}
$$

Difference in average of $X$ between the two groups $141.86 - 135.33 = 6.53 \text{mmHg}$. 

*Clearly overall there has been some reduction in systolic blood pressure for those in the Captopril arm, but how statistically sound is this as evidence? It could be that really (for the hypothetical population) there is no reduction, and we have just been 'lucky'.* 

The variances within the two groups are fairly close, so we can use the pooled estimate of standard deviation:
  
$$
  s_p = \sqrt{\frac{\sum\limits_{i=1}^N\left(n_i-1\right)s_i^2}{\sum\limits_{i-1}^N\left(n_i-1\right)}}.
$$
  
  In our case 

$$ 
  \begin{aligned}
s_p&= \sqrt{\frac{8\times{8.43^2} + 6 \times{6.94^2}}{8+6}}\\
& = 7.82\text{ mmHg.}
\end{aligned}
$$

We can do an independent two-sample $t$-test,

$$
  \begin{aligned}
t & = \frac{\bar{X_C} - \bar{X_T}}{s_p\sqrt{\frac{1}{n_C} + \frac{1}{n_T}}}\\
& = \frac{6.53}{7.82\sqrt{\frac{1}{7} + \frac{1}{9}}} \\
& = 1.65.
\end{aligned}
$$
Note that here the placebo group is group $C$, and the Captopril group is group $T$.

Under $H_0$, this value should be $t_{14}$ ($n_i-1$ for each group). 

```{r ttestcapt, fig.cap = "The distribution $t_{14}$, with $t=1.65$ shown by the dashed line and the 'more extreme' areas shaded. "}
x1 = seq(-4, -1.65, by=0.01)
px1 = dt(x1, df=14)
x2 = seq(1.65,4, by=0.01)
px2 = dt(x2, df=14)

ggplot(data=data.frame(x=c(-4,4)), aes(x)) +
  stat_function(fun=dt, args=list(df=14))+
  geom_ribbon(data=data.frame(x=x1, y=px1), aes(x=x, ymin=0, ymax=y), fill="red", alpha=0.4) + 
  geom_ribbon(data=data.frame(x=x2, y=px2), aes(x=x, ymin=0, ymax=y), fill="red", alpha=0.4) +xlab("Test statistic (t)") + ylab("Density") +
  geom_vline(xintercept = 1.65, lty=2) + theme_bw()
```

*The dashed line is at $t=1.65$, and the red shaded areas show anywhere 'at least as extreme'. We can find the area (ie. the probability of anything at least as extreme as our found value) in R by *

```{r echo=T}
2*(1-pt(1.65, df=14))
```

So $p=0.121$ - not significant even at $\alpha=0.1$.
:::
  
### What do we do with this outcome?
  
Worst case scenario:

  * $\hat{\tau} = 6.53\text{ mmHg}$ is large enough to be compelling 
  * Dataset is too small for it to be statistically significant
  * Can't confidently conclude that Captopril has any effect on blood pressure (reject $H_0$). 
  * Can't say that there is no effect. 
  
*This is exactly the sort of scenario we hoped to avoid when planning our study.*

Consider the range of treatment effects that are compatible with our trial data. That is, we find the set

$$\left\lbrace \tau \mid \frac{\lvert \bar{x}_C - \bar{x}_T - \tau \rvert}{s\sqrt{n_C^{-1} + n_T^{-1}}} \leq t_{n_C+n_T-2;\,0.975} \right\rbrace. $$
Suppose the true treatment effect is  $\tau^*$, and we test $H_0:\; \tau = \tau^*$. 

  * For all $\tau^*$ inside this range, our data are not sufficiently unlikely to reject the $H_0$ at the 0.05 level. 
  * For all values of $\tau^*$ outside this range, our data are sufficiently unlikely to reject that hypothesis. 
  
Rearrange to give a 95% confidence interval for $\tau$,

$$\left\lbrace \tau \mid \bar{x}_C - \bar{x}_T - t_{n_C+n_T-2;\,0.975}\,s\sqrt{n_C^{-1} + n_T^{-1}} \leq \tau \leq \bar{x}_C - \bar{x}_T + t_{n_C+n_T-2;\,0.975}\,s\sqrt{n_C^{-1} + n_T^{-1}}  \right\rbrace $$

::: {.example}

Continuing our example, we have

$$\left\lbrace \tau \mid \frac{\lvert 6.53 - \tau \rvert}{7.82\sqrt{\frac{1}{7} + \frac{1}{9}}} \leq t_{14;0.975} = 2.145 \right\rbrace $$
  
*Here, $t_{14;0.975} = 2.145$ is the $t$-value for a significance level of $0.05$, so if we were working to a different significance level we would change this.*

Rearranging as above, this works out to be the interval

$$
-1.92 \leq  \tau \leq 14.98.
$$

Notice that zero is in this interval, consistent with the fact that we failed to reject the null hypothesis.
:::

Some things to note

  * We can compute this confidence interval whether or not we failed to reject the null hypothesis that $\tau=0$, and for significance levels other than 0.05.
  * Reporting the CI is much more informative than simply reporting the $P$-value. *In our Captopril example, we found that a negative treatment effect (ie. Captopril reducing blood pressure less than the placebo) of more than 2 mmHg was very unlikely, whereas a positive effective (Captopril reducing blood pressure) of up to 15 mmHg was plausible. If Captopril were inexpensive and had very limited side effects (sadly neither of which is true) it may still be an attractive drug.*
  * *These confidence intervals are exactly the same as you have learned before, but we emphasise them because they are very informative in randomised controlled trials (but not so often used!).*

*At the post trial stage, when we have data, the confidence interval is the most useful link to the concept of *power*, which we thought about at the planning stage.* 

Remember that the power function is defined as 

$$\psi \left(\tau\right) = P\left(\text{Reject }H_0\mid \tau\neq 0\right),$$ 

*This was calculated in terms of the theoretical model of the trial, and in terms of some minimum detectable effect size $\tau_M$ that we wanted to be able to correctly detect with probability $1-\beta$ (the power). Sometimes people attempt to re-calculate the power after the trial, to detect whether the trial was underpowered. However, now we have actual data.*

You can't calculate the power of a trial after it's happened, but if we fail to reject $H_0$ and $\tau_M$ is in the confidence interval for $\tau$, then that is a good indication that our trial was indeed underpowered.

## (Lecture 8) Using baseline values {#baseline}

*In our example above, our primary outcome variable $X$ was the systolic blood pressure of each participant at the end of the intervention period.*

In Table \@ref(tab:captoprildata) we also have *baseline* measurements: measurements of systolic blood pressure for each patient from before the intervention period. We'll denote these $B_T$ and $B_C$.

Baseline measurements are useful primarily for two reasons:

  1. They can be used to assess the balance of the design. 
  2. They can be used in the analysis.
  
We will demonstrate these by returning to our Captopril example.

::: {.example}

Balance:

  * Placebo group: $\bar{b}_C = 146.6 \text{ mmHg}$ and $SD\left(b_C\right) = 12.3 \text{ mmHg}$
  * Captopril group: $\bar{b}_T = 148 \text{ mmHg}$ and $SD\left(b_T\right) = 11.4 \text{ mmHg}$
  
Not identical, but sufficiently similar not to suspect systematic imbalance. In a study this small there is likely to be some difference.

*Note, it's sensible to compare things informally like this, but sometimes people suggest using a t-test or similar to test whether the two groups are similar enough. This is a flawed exercise: a formal hypothesis test is testing whether what we see is likely to have arisen by random change. We  know that this arose by random chance, because we randomly allocated the patients! A formal test only makes sense if you suspect that something has systematically disrupted the randomisation.*

Analysis:

  * Interested in whether Captopril has reduced BP for each individual - makes sense to compare change in BP ($X-B$), rather than final BP measurement.

We can see individual data in Table \@ref(tab:captoprildiff) and summary statistics in Table \@ref(tab:captoprildiffsumm).

```{r captoprildiff}
df_hommel_diff = df_hommel
df_hommel_diff$diff = df_hommel_diff$outcome - df_hommel_diff$baseline
df_hommel_diff_print = df_hommel_diff
names(df_hommel_diff_print) = c("Patient (ID)", "Baseline (B)", "Outcome at 1 week (X)", "Trial Arm", "Difference")

knitr::kable(df_hommel_diff_print, caption = "Data for the Captopril trial from @hommel1986effect, with differences shown.")
```

```{r captoprildiffsumm}
df_hommel2 = data.frame(
  Sam = c(9,7),
  Mean = c(
    mean(df_hommel_diff$diff[df_hommel_diff$arm=="Captopril"]),
    mean(df_hommel_diff$diff[df_hommel_diff$arm=="Placebo"])),
  SD = c(
    sd(df_hommel_diff$diff[df_hommel_diff$arm=="Captopril"]),
    sd(df_hommel_diff$diff[df_hommel_diff$arm=="Placebo"]))
)
df_hommel2$SEmean = df_hommel2$SD / sqrt(df_hommel2$Sam) 
df_hommel2_print=  round(df_hommel2,2)
names(df_hommel2_print) = c("Sample Size", "Mean (mmHg)", "SD (mmHg)", "SE of mean (mmHg)")
row.names(df_hommel2_print) = c("Captopril", "Placebo")

knitr::kable(df_hommel2_print, caption = "Summary statistics for each group.")
```

Now we can perform our test as before, 

$$ t = \frac{- 12.67 - (-4.71)}{8.54\sqrt{\frac{1}{7}+\frac{1}{9}}} = -1.850 $$
where 8.54 is the pooled standard deviation (as before). Under the null distribution of no difference, this has a $t$-distribution with 14 degrees of freedom, and so we have a $P$-value of 0.086. Our 0.95 confidence interval is 

$$ -12.67 - (-4.71) \pm t_{14;\,0.975}\times 8.54\sqrt{\frac{1}{7}+\frac{1}{9}} = \left[-17.2,\;1.3\right].$$
Taking into account the baseline values in this way has 

  * slightly reduced the $P$-value (though still $p>0.05$)
  * shifted the confidence interval slightly lower 

:::

### Why did the CI and p-value change?

Notation:

  * Baseline: $B_C,\,B_T$
  * Outcome : $X_C,\,X_T$
  
All participants have been randomised from the same population, so 

$$\operatorname{E}\left(B_C\right) = \operatorname{E}\left(B_T\right) = \mu_B.$$
Assuming some treatment effect $\tau$ (which could still be zero) we have

$$
\begin{aligned}
\operatorname{E}\left(X_C\right) & = \mu\\
\operatorname{E}\left(X_T\right) & = \mu + \tau.
\end{aligned}
$$
*Sometimes we'll have $\mu_B=\mu$, but this won't always be the case.* 

Usually we will assume that

$$\operatorname{Var}\left(X_C\right) = \operatorname{Var}\left(X_T\right) = \operatorname{Var}\left(B_C\right) = \operatorname{Var}\left(B_T\right) = \sigma^2,$$
*and this is generally fairly reasonable in practice. *

For our two analyses so far we have

$$
\begin{aligned}
\operatorname{E}\left(X_T\right) - \operatorname{E}\left(X_C\right) & = \left(\mu + \tau\right) - \mu = \tau\\
\operatorname{E}\left(X_T - B_T\right) - \operatorname{E}\left(X_C - B_C\right) & = \left(\mu - \mu_B + \tau\right) - \left(\mu - \mu_B\right) = \tau,
\end{aligned}
$$
that is, both are unbiased estimators of $\tau$.

However, whereas the first is based on data with variance $\sigma^2$, the second has

$$
\begin{aligned}
\operatorname{Var}\left(X_T-B_T\right) & = \operatorname{Var}\left(X_T\right) + \operatorname{Var}\left(B_T\right) - 2\operatorname{cov}\left(X_T,B_T\right)\\
& = \sigma^2 + \sigma^2 - 2\rho\sigma^2 \\
& = 2\sigma^2\left(1-\rho\right),
\end{aligned}
$$
where $\rho$ is the true correlation between $X$ and $B$, and is assumed to be the same in either group.

Similarly, 

$$\operatorname{var}\left(X_C-B_C\right) = 2\sigma^2\left(1-\rho\right).$$ 


Using this to work out the variance of the estimator $\hat{\tau}$ we find that for comparing means we have

$$\operatorname{var}\left(\tau\right) = \operatorname{var}\left(\bar{x}_T - \bar{x}_C\right) = \sigma^2\left(\frac{1}{m}+\frac{1}{n}\right).$$

whereas for comparing differences from baseline

$$\operatorname{var}\left(\tau\right) = \operatorname{var}\left[\left(\overline{X_T-B_T}\right) - \left(\overline{X_C - B_C}\right)\right] = 2\sigma^2\left(1-\rho\right)\left(\frac{1}{m}+\frac{1}{n}\right).$$


Therefore, 

  * If $\frac{1}{2}<\rho\leq 1$ there will be a smaller variance when comparing differences
  * If $0\leq\rho<\frac{1}{2}$there will be a smaller variance when comparing outcome variables

*Intuitively, this seems reasonable: if the correlation between baseline and outcome measurements is very strong, then we can remove some of the variability between participants by taking into account their baseline measurement. However, if the correlation is weak, then by including the baseline in the analysis we are essentially just introducing noise.*

For our Captopril example, the sample correlation between baseline and outcome is 0.63 in the Captopril group and 0.80 in the Placebo group. This fits with the $P$-value having reduced slightly.

### A dodgy way to use baseline variables 

*Sometimes the analysis performed on a dataset is rather spurious, but it isn't always immediately obvious why. We'll look at one example now, because it is done sometimes.*

Look at each group separately and determine whether there has been a significant change in the outcome variable (assumes $\mu_B = \mu$).

For Captopril data, we could perform a paired $t$-test on the difference between baseline $B$ and outcome $X$ for each patient, for each group.

If we do this, we find the summary statistics in Table \@ref(tab:ttestdodge).

```{r ttestdodge}
df_dodge = data.frame(
  t_stat = c(4.23, 1.58),
  df = c(8,6),
  p_value = c(0.003, 0.17)
  )
row.names(df_dodge) = c("Captopril", "Placebo")

knitr::kable(df_dodge, caption = "Summary statistics for the dodgy analysis")

```

We find

  * Strong evidence for a change in blood pressure for the Captopril patients (group $T$)
  * No such evidence for the placebo patients. 
  
Can we therefore conclude that Captopril is significantly better than the placebo? No! The analysis is flawed:

  * The $p$-value of 0.17 in the control group doesn't show that $H_0$ is true *(no treatment effect for the control group) is true, just that we can't reject the null hypothesis. It is quite possible that there is a difference in the control group, and that numerically it could even be comparable to that in the treatment group, so although we can say that there is a significant reduction in blood pressure for the captopril group, we can't conclude that Captopril is better than the placebo.*
  * Having set up the experiment as a randomised controlled trial, with a view to comparing the two groups, it seems strange to then deal with them separately.


## Analysis of covariance (ANCOVA)

Previously: based our analysis on the baseline values being statistically identical draws from the underlying distribution, and therefore having the same expectation and variance. 

However, in the event there will be some imbalance.

We can see this in our Captopril example. Eg. we saw that difference in baseline means is $\bar{b}_C - \bar{b}_T = 1.4 \text{ mmHg}$.

  * Not clinicaly significant
  * Not large enough to make us doubt our randomisation
  * A difference nonetheless


```{r hommel, echo=F, fig.cap = "Baseline measurements from the Captopril trial."}

ggplot(data = df_hommel, aes(y=baseline, fill = arm)) + geom_boxplot() + ylab("Baseline (mmHg)")+theme_bw()

```

Basic principle of ANCOVA: if there is some correlation between baseline and outcome, then baselines differing leads to outcomes differing, even if there is no treatment effect (ie. if $\tau=0$). 

*Indeed, how do we decide how much of the difference in outcome is down to the treatment itself, and how much is simply the difference arising from different samples?* 

*This issue arises in many trials, particularly where there is a strong correlation between baseline and outcome measurements.*

### The theory {#ancovatheory}

Usual notation and assumptions:

  * Outcome $X$, baseline $B$
  * $E\left(X_T\right) = \mu + \tau$
  * $E\left(X_C\right) = \mu$
  * $var\left(X_T\right) = var(X_C) = \sigma^2$
  * Correlation between $B$ and $X$ is $\rho$ **within each group**
  * We want to learn about $\tau$

Baseline $B$ (same measurement as $X$) is measured before start of trial. 
Assume $E(B_T) = E(B_C) = \mu_B$ and $var(B_C) = var(B_T) = \sigma^2$.
Also assume both groups have size $N$, therefore $2N$ patients in all.

We observe baseline measurements $b_1,\,b_2,\ldots,b_{2N}$. 

Given these values, we have

$$
\begin{aligned}
\operatorname{E}\left(X_i\mid{b_i}\right) &= \mu + \rho\left(b_i - \mu_B\right)\text{ in the control group}\\
\operatorname{E}\left(X_i\mid{b_i}\right) &= \mu +\tau + \rho\left(b_i - \mu_B\right)\text{ in the test group.}
\end{aligned}
$$

From this, we find that

\begin{equation}
\operatorname{E}\left(\bar{X}_T - \bar{X}_C\mid{\bar{b}_T,\,\bar{b}_C}\right) = \tau + \rho\left(\bar{b}_T - \bar{b}_C\right). 
(\#eq:diffexp)
\end{equation}

If there is a difference in the baseline mean between groups, then the difference in outcome means is not an unbiased estimator of the treatment effect $\tau$. 

Assuming $\rho>0$ (which is almost always the case) then 

  * if $\bar{b}_T>\bar{b}_C$ the difference in outcome means overestimates $\tau$
  * if $\bar{b}_T<\bar{b}_C$, the difference in outcome means underestimates $\tau$. 
  
*The only situation in which the difference in outcome means is an unbiased estimator is when $\rho=0$, however this is not common in practice.*

Comparing the difference between outcome and baseline, as we did in \@ref(baseline), does not solve this problem, since we have

$$\operatorname{E}\left[\left(\bar{X}_T - \bar{b}_T\right) - \left(\bar{X}_C - \bar{b}_C\right)\mid{\bar{b}_T,\,\bar{b}_C}\right] = \tau + \left(\rho-1\right)\left(\bar{b}_T - \bar{b}_C\right),$$ 
which is similarly unbiased (unless $\rho=1$, which is never the case).

Notice, however, that if we use as our estimator 

$$\left(\bar{X}_T - \bar{X}_C\right) - \rho \left(\bar{b}_T - \bar{b}_C\right)$$ then, following from Equation \@ref(eq:diffexp) we have

$$\operatorname{E}\left[\left(\bar{X}_T - \bar{X}_C\right) - \rho \left(\bar{b}_T - \bar{b}_C\right)\mid{\bar{b}_T,\,\bar{b}_C}\right] = \tau + \rho\left(\bar{b}_T - \bar{b}_C\right)- \rho\left(\bar{b}_T - \bar{b}_C\right) = \tau. $$
### What's the variance of this estimator? {-}



### The practice

In the previous section we established an unbiased estimate of the treatment effect that takes into account the baseline measurements. However, we can't use it as a model, because there are a few practical barriers:

  * Our estimate for $\tau$ relies on the correlation $\lambda$, which is unknown
  * In real life, the groups are unlikely to have equal size and variance, so ideally we'd lose these constraints
  
We can solve both of these by fitting the following statistical model to the observed outcomes $x_i$:

$$
\begin{aligned}
x_i & = \mu + \gamma b_i + \epsilon_i & \text{ in group C}\\
x_i & = \mu + \tau + \gamma b_i + \epsilon_i & \text{ in group T}&.
\end{aligned}
$$
Here, the $\epsilon_i$ are independent errors with distribution $N\left(0,\,\sigma^2\right)$, the $b_i$ are the baseline measurements for $i=1,\ldots,N_T+N_C$, for groups $T$ and $C$ with sizes $N_T$ and $N_C$ respectively. Sometimes this is written instead in the form 

$$ x_i = \mu + \tau G_i+ \gamma b_i + \epsilon_i $$
where $G_i$ is 1 if participant $i$ is in group $T$ and 0 if they're in group $C$. This is a factor variable, which you may remember from Stats Modelling II (if you took it). If $G_i=1$ (ie. participant $i$ is in group $T$) then $\tau$ is added. If $G_i=0$ (ie. participant $i$ is in group $C$) then it isn't. 

We now have four parameters to estimate: $\mu,\,\tau,\,\gamma$ and $\sigma^2$. For the first three we can use least squares (as you have probably seen for linear regression). Our aim is to minimise the sum of squares

$$S\left(\mu,\, \tau,\,\gamma\right) = \sum\limits_{i\text{ in }T} \left(x_i - \mu - \tau - \gamma b_i\right)^2 + \sum\limits_{i\text{ in }C} \left(x_i - \mu - \gamma b_i\right)^2.$$

This leads to estimates $\hat{\mu},\, \hat{\tau}$ and $\hat{\gamma}$. We won't worry about how this sum is minimised, since we'll always be using pre-written R functions. We can use the estimates $\hat{\mu},\, \hat{\tau}$ and $\hat{\gamma}$ to estimate $\sigma^2$, using

$$\hat{\sigma}^2 = \frac{S\left(\hat{\mu},\hat{\tau}, \hat{\gamma}\right)}{N_T + N_C -3}.$$
The general form for this is 

$$ \hat{\sigma}^2 = \frac{SSE}{n-p},$$
where $SSE$ is the residual sum of squares, $n$ is the number of data points and $p$ the number of parameters (apart from $\sigma^2$) being estimated. If you want to know why that is, you can find out [here](https://pages.stern.nyu.edu/~wgreene/MathStat/GreeneChapter4.pdf) (look particularly at page 62), but we will just take it as given!

As well as generating a fitted value $\hat{\tau}$, we (or rather R!) will also find the standard error of $\hat\tau$, and we can use this to generate a confidence interval for the treatment effect $\tau$.

The technique described above is a well-established statistical method known as **ANCOVA** (short for the **An**alysis of **Cova**riance), which can be implemented in R and many other statistical software packages.

::: {.example}
Let's now implement ANCOVA on our Captopril data in R.
We do this by first fitting a linear model using 'lm', with baseline measurement and arm as predictor variables and outcome as the predictand. 
```{r, echo=T}
# Gives same answers as above, so use this!
# Find out what the numbesrs mean
# Background on ANOVA and factor models (SMII)
# Where to include random effects models? Is it in Matthews?
lm_capt = lm(outcome ~ baseline + arm, data = df_hommel)
summary(lm_capt)
```

The variable 'arm' here is being included as a factor variable, so it behaves like

$$
\text{arm}_i =
\begin{cases}
0 & \text{ if participant }i\text{ is assigned Captopril}\\
1 & \text{ if participant }i\text{ is assigned Placebo}.
\end{cases}
$$
Therefore, for a patient assigned Placebo, a value of 7.1779 is added, as well as the intercept and baseline term. This results in a model with two parallel fitted lines.

```{r}
ggplot(data=df_hommel, aes(x=baseline, y=outcome, col = arm)) + 
  geom_point() +
  geom_abline(
    slope = coef(lm_capt)[["baseline"]],
    intercept = coef(lm_capt)[["(Intercept)"]],
    colour = "red") +
  geom_abline(
    slope = coef(lm_capt)[["baseline"]],
    intercept = coef(lm_capt)[["(Intercept)"]] + coef(lm_capt)[["armPlacebo"]],
    colour = "darkturquoise")+theme_bw()
```

For our previous methods we have calculated a confidence interval for the treatment effect $\tau$, and we will do that here too. The second column of the linear model summary (above) gives the standard errors of each estimated parameter, and we see that the standard error of $\hat{\tau}$ is 2.9636. Therefore, to construct a 95/% confidence interval for $\hat{\tau}$, we use (to 3 decimal places)

$7.178\; \pm\; t_{0.975;13}\times{2.964}  = \left(0.775,\; 13.580\right).$

The model has $n-p=13$ degrees of freedom because there are $n=16$ data points and are estimating $p=3$ parameters. 
Notice that unlike our previous confidence intervals, this doesn't contain zero, and so our analysis has enabled us to conclude that there is a significant reduction in blood pressure with Captopril. However, you can tell from the width of the interval that there is still a lot of uncertainty about $\tau$.

The 'Residual standard error' term near the bottom of the linear model summary is the estimate of $\hat{\sigma}$, so here we have $\hat{\sigma}^2 = 5.869^2 = 34.44.$

As with any fitted model, we should check the residuals.

```{r, echo=T}
resid_capt = resid(lm_capt)
df_hommel$resid= resid_capt

ggplot(data = df_hommel, aes(x=baseline, y=resid, col=arm)) + 
  geom_point() +
  geom_hline(yintercept=0)+
  xlab("Baseline")+
  ylab("Residual")+theme_bw()
```

These look pretty good; there are no clear patterns and the distribution appears to be similar for each treatment group. Though, with such a small sample it's difficult really to assess the fit of the model. In our practical, we'll work with a more realistic dataset.

:::

## Some follow-up questions....

This might have raised a few questions, so we will address those now.

### Didn't we say that $X_T - X_C$ was an unbiased estimator of $\tau$?

In Sections \@ref(ttest) and \@ref(baseline) we used both $\bar{X}_T - \bar{X}_C$ and $\left(\bar{X}_T - \bar{B}_T\right) - \left(\bar{X}_C - \bar{B}_C\right)$ as unbiased estimators of $\tau$.
Then, in Section \@ref(ancovatheory) we showed that 

$$
\begin{aligned}
\operatorname{E}\left(\bar{X}_T - \bar{X}_C\mid{\bar{b}_T,\;\bar{b}_C}\right)& = \tau + \rho\left(\bar{b}_T - \bar{b}_C\right)\\
\operatorname{E}\left[\left(\bar{X}_T - \bar{b}_T\right) - \left(\bar{X}_C - \bar{b}_C\right)\mid{\bar{b}_T,\,\bar{b}_C}\right] &= \tau + \left(\rho-1\right)\left(\bar{b}_T - \bar{b}_C\right),
\end{aligned}
$$
that is, neither of these quantities are unbiased estimators of $\tau$ (except in very specific circumstances). 

Is this a contradiction? 

You'll be relieved to hear (and may already have realised) that it isn't; the first pair of equations are blind to the baseline values $B_T$ and $B_C$, and are using their statistical properties. Because of the randomisation procedure, a priori they can be treated the same. However, once we have observed values for the baseline, $b_T$ and $b_C$, they are very unlikely to be exactly the same. They are also (along with all other baseline measurements, often things like age, sex, height etc.) definitely not affected by the trial, since they are taken before any placebo or treatment has been administered, and often even before allocation. However, conditioning on their observed values can reduce the variance of our estimate of $\tau$, as we have seen.

In this sense, the observed baseline means $\bar{b}_T$ and $\bar{b}_C$ are known as **ancillary statistics**; they contain no direct information about the parameter we are interested in (in this case the treatment effect $\tau$), but our inferences can be improved by conditioning on the observed values of the ancillary statistics.


### What if the lines shouldn't be parallel? The unequal slopes model

In the analysis above, we have assumed that the coefficient $\gamma$ of baseline (the estimate of the correlation between outcome and baseline) is the same in both groups; we have fitted an **equal slopes model**. It isn't obvious that this should be the case, and indeed we can test for it.

Allowing each group to have a different slope means including an interaction term between baseline and treatment group, 

$$ x_i = \mu + \tau G_i+ \gamma b_i + \lambda b_i G_i + \epsilon_i . $$
The term $\lambda b_i G_i$ is 0 if participant $i$ is in group $C$ and $\lambda b_i$ if participant $i$ is in group $T$. Therefore, for participants in group $C$, the gradient is still $\gamma$, but for participants in group $T$ it is now $\gamma + \lambda$. We can test whether this interaction term should be included (that is, whether we should fit an unequal slopes model) by including it in a model and analysing the results.

:::{.example}

Continuing once again with the Captopril dataset, we now fit the model

```{r, echo=T}
lm_capt_int = lm(outcome ~ arm + baseline + baseline:arm, data = df_hommel)
summary(lm_capt_int)
anova(lm_capt_int) # still don't think I need to include this
```
We see that the $p$-value for the coefficient $\lambda$ (seen in the `arm:baseline` row) is not at all significant (0.97). Therefore we can be confident that there is no need to fit unequal slopes for this dataset. This fits with our earlier conclusion (from inspecting the residuals) that just including first order terms is fine.
:::



### Can we include any other baseline covariates?

In Section \@ref(baseline) when our estimated treatment effect was $\hat\tau = \left(\bar{x}_T - \bar{b}_T\right) - \left(\bar{x}_C - \bar{b}_C\right)$, we could the only other variable we could take into account was the baseline measurement, because it is on the same scale as the outcome $X$. However, in ANCOVA, our treatment effect is 

$$ \hat\tau = \left(\bar{x}_T - \bar{x}_C\right) - \hat\gamma\left(\bar{b}_T - \bar{b}_C\right), $$
and the inclusion of the coefficient $\gamma$ means that we can include other covariates on different scales too. The key issue is that we can only include as covariates things that were already known before allocation (hence they are sometimes known as *baseline covariates*, not to be confused with 'the baseline', which would generally mean the same measurement as the primary outcome, but before treatment). This is because they cannot, at that point, have been affected by the treatment, or have had an influence on the post-trial outcome measurement. Indeed, as a rule, any variable that was used in the randomisation procedure (this particularly applies to minimisation and stratified sampling) should be included in the analysis.

::: {.example}
The data for this example is taken from @datarium. In this study, 60 patients take part in a trial investigating the effect of a new treatment and exercise on their stress score, after adjusting for age.
There are two treatment levels (yes or no) and three exercise levels (low, moderate and high) and 10 participants for each combination of treatment and exercise levels. Because in ANCOVA we fit a coefficient to every covariate, we can include exercise (another factor variable) and age (a continuous variable) in this analysis.

```{r}
library(datarium)
library(rmarkdown)
paged_table(stress)
```

The table below shows the mean and standard deviation of age for each combination of treatment and exercise level. If we were being picky / thorough, we might note that (perhaps unsurprisingly!) the mean and standard deviation of age are both lower in the high exercise groups. This might well affect our analysis, but we won't go into this now.

```{r}
library(dplyr)
summary_stress = stress %>% group_by(treatment, exercise) %>% dplyr::summarize(mean = mean(age), sd= sd(age))
knitr::kable(summary_stress)
```

Fitting a linear model, we see that treatment, high levels of exercise and age have an effect on stress.

```{r, echo=T}
lm_stresslin = lm(score ~ treatment + exercise + age, data = stress)
summary(lm_stresslin)

```

In particular, taking a high level of exercise reduced participants' stress scores by around 9.6, and the treatment reduced stress scores by around 4.3. Participants' stress scores increased slightly with age (just under half a point per year!). 

We can plot the residuals to check that the model is a reasonable fit

```{r}
stress_sum = data.frame(
  resid = resid(lm_stresslin),
  fitted = fitted(lm_stresslin),
  treatment = stress$treatment
)
ggplot(data=stress_sum, aes(x=fitted, y=resid, col=treatment)) +
  geom_point() +
  xlab("Fitted value") +
  ylab("Residual")+
  geom_hline(yintercept=0, lty=2)+theme_bw()


```

:::


## Some general principles of Analysis

There are some assumptions we're making here, and so we need to be careful when fitting an ANCOVA model.

  * We're assuming that the residual variance is the same for both groups
  * We're assuming that the coefficient of the baseline is the same for both groups: only the intercept is changing.
  
We can't check the first until after 

::: {.example}

Before fitting our ANCOVA model we should first have checked that there's no significant relationship between the covariate (in this case the baseline) and the treatment group, which we can do using ANOVA:

```{r, echo=T}
# There's probably a better way to do this - check online articles
model_aov_capt = aov(baseline ~ arm, data = df_hommel)
summary(model_aov_capt)
```

If the experiment has been properly designed then there shouldn't be, but this is not always the case, and indeed depending on how we do the randomisation we could be unlucky. If baseline measurements are available before allocation, this can be checked before the trial is run.

:::
